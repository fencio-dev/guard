# MCP Gateway Intelligence Layer Design

**Date:** 2025-11-18
**Status:** Design Complete - Ready for Implementation
**Goal:** Reduce token usage by 70%+ through automatic summarization and semantic caching

## Problem

Claude Code users receive raw MCP tool results, often consuming 10-50KB of context per query. Repeated similar queries waste API calls. Users need intelligent summarization and semantic search over past results.

## Solution

Add an intelligence layer that automatically:
1. Summarizes results ≥2KB using Gemini 2.5 Flash
2. Caches semantically similar queries (3-hour TTL)
3. Indexes summaries in ChromaDB for semantic search
4. Exposes RAG tools for context assembly

## Architecture

### Post-Processing Middleware

The intelligence layer intercepts results in `sandbox.execute()`:

```
run_code execution completes
    ↓
Check result size ≥2KB?
    ↓ YES
Generate embedding (ChromaDB)
    ↓
Check semantic cache (cosine similarity >0.9)
    ↓ CACHE HIT: return cached summary
    ↓ CACHE MISS: call Gemini
Summarize with structured output
    ↓
Store artifact + summary + vector
    ↓
Return summary + artifact reference
```

**On failure:** Return raw result (graceful degradation). Never block users.

### Components

1. **Sandbox Middleware** (`src/runtime/sandbox.ts`)
   - Intercepts execution results
   - Triggers intelligence processing at 2KB threshold
   - Handles failures gracefully

2. **Gemini Client** (`@google/genai` SDK)
   - Model: `gemini-2.5-flash`
   - Structured output via `responseMimeType: 'application/json'`
   - Prompt templates in `src/prompts/summarize.ts`

3. **ChromaDB Integration** (`chromadb` npm package)
   - Collection: `mcp_artifacts`
   - Embeddings for semantic similarity
   - 3-hour TTL (matches artifact expiration)

4. **MCP Tools** (`src/mcp-server/intelligence-tools.ts`)
   - `semantic_search({query, limit})` - Find past results via NLP
   - `get_artifact_context({ids, maxTokens})` - Assemble focused context

5. **Slash Command** (`.claude/commands/gateway-init.md`)
   - Injects complete usage instructions into `.claude/CLAUDE.md`
   - Self-contained guide (≤300 tokens)
   - No resource references

## Data Structures

### Summary Schema (Gemini Output)

```typescript
interface SummaryOutput {
  keyFindings: string[];           // 3-5 bullet points
  actionableInsights: string[];    // What Claude can do with this data
  metadata: {
    toolName: string;
    serverId: string;
    timestamp: string;
    artifactId: string;
  };
}
```

### ChromaDB Collection

```typescript
{
  id: string;                  // UUID
  embedding: number[];         // Generated by ChromaDB
  metadata: {
    serverId: string;
    toolName: string;
    timestamp: number;
    artifactPath: string;
    summaryText: string;       // JSON.stringify(SummaryOutput)
    tags: string[];
  };
  document: string;            // Summary text for embedding
}
```

## Implementation

### Dependencies

```json
{
  "@google/genai": "^0.21.0",
  "chromadb": "^1.9.2"
}
```

### Environment Variables

```bash
# Required
GEMINI_API_KEY=your_key_here

# Optional (defaults shown)
MCP_GATEWAY_INTELLIGENCE_ENABLED=true
MCP_GATEWAY_SUMMARY_THRESHOLD_BYTES=2048
MCP_GATEWAY_CHROMA_URL=http://localhost:8001
```

### Files Changed

**Modified:**
- `package.json` - Add dependencies
- `src/runtime/sandbox.ts` - Add intelligence middleware
- `src/runtime/sandbox.test.ts` - Add intelligence tests
- `src/gateway.ts` - Initialize ChromaDB client
- `src/mcp-server/server.ts` - Add tool handlers

**New:**
- `src/prompts/summarize.ts` - Gemini prompt templates
- `src/mcp-server/intelligence-tools.ts` - Tool definitions
- `src/mcp-server/commands/gateway-init.ts` - Slash command
- `.claude/commands/gateway-init.md` - Command registration
- `tests/intelligence-e2e.test.ts` - Integration tests

### Core Logic

**Sandbox Middleware** (`src/runtime/sandbox.ts`):

```typescript
async execute(code: string, options: SandboxOptions): Promise<SandboxOutput> {
  const result = await vm.run(wrappedCode);

  // Intelligence layer
  if (this.intelligenceEnabled && result) {
    const resultStr = JSON.stringify(result);

    if (resultStr.length >= this.summaryThreshold) {
      try {
        const processed = await this.processWithIntelligence(result, metadata);
        return {
          result: processed.summary,
          logs: [...logs, `[Intelligence] Saved to ${processed.artifactId}`],
          executionTime: Date.now() - startTime
        };
      } catch (error) {
        console.error('[Intelligence] Failed, returning raw:', error);
        // Fall through to return raw result
      }
    }
  }

  // Existing size limit enforcement
  return { result, logs, executionTime };
}

private async processWithIntelligence(result: unknown, metadata: any) {
  // 1. Generate embedding
  const text = JSON.stringify(result);
  const embedding = await this.chromaClient.createEmbedding(text);

  // 2. Check semantic cache
  const similar = await this.collection.query({
    queryEmbeddings: [embedding],
    nResults: 1,
    where: { timestamp: { $gte: Date.now() - 3 * 60 * 60 * 1000 } }
  });

  if (similar.distances[0][0] < 0.1) {
    return { summary: JSON.parse(similar.metadatas[0][0].summaryText), artifactId: similar.ids[0][0] };
  }

  // 3. Summarize with Gemini
  const model = this.geminiClient.getGenerativeModel({
    model: 'gemini-2.5-flash',
    generationConfig: { responseMimeType: 'application/json' }
  });

  const prompt = buildSummaryPrompt({ data: result, ...metadata });
  const response = await model.generateContent(prompt);
  const summary: SummaryOutput = JSON.parse(response.response.text());

  // 4. Store artifact and index
  const artifactId = await this.saveArtifact(result, summary);
  await this.collection.add({
    ids: [artifactId],
    embeddings: [embedding],
    metadatas: [{ summaryText: JSON.stringify(summary), ...metadata }],
    documents: [JSON.stringify(summary)]
  });

  return { summary, artifactId };
}
```

**Prompt Template** (`src/prompts/summarize.ts`):

```typescript
export function buildSummaryPrompt(input: SummaryInput): string {
  return `Summarize this MCP tool result.

Tool: ${input.serverId}/${input.toolName}
Data: ${JSON.stringify(input.data)}

Generate structured JSON:
{
  "keyFindings": ["finding1", "finding2", "finding3"],
  "actionableInsights": ["insight1", "insight2"],
  "metadata": {
    "toolName": "${input.toolName}",
    "serverId": "${input.serverId}",
    "timestamp": "${new Date().toISOString()}",
    "artifactId": "${input.artifactId}"
  }
}

Keep findings under 500 words total.`;
}
```

**MCP Tool Handlers** (`src/mcp-server/server.ts`):

```typescript
// semantic_search
const embedding = await this.chromaClient.createEmbedding(query);
const results = await this.collection.query({
  queryEmbeddings: [embedding],
  nResults: limit || 5,
  where: { timestamp: { $gte: Date.now() - TTL_MS } }
});

return results.metadatas.map((m, i) => ({
  artifactId: m.artifactId,
  summary: JSON.parse(m.summaryText),
  relevanceScore: 1 - results.distances[i][0],
  path: m.artifactPath
}));

// get_artifact_context
let totalTokens = 0;
const contexts = [];

for (const id of ids) {
  const artifact = await this.readArtifact(id);
  const tokens = this.estimateTokens(JSON.stringify(artifact.summary));

  if (totalTokens + tokens <= maxTokens) {
    contexts.push(artifact.summary);
    totalTokens += tokens;
  } else {
    break;
  }
}

return { contexts, totalTokens };
```

## Testing

### Unit Tests

**Summarization:**
- Results ≥2KB trigger summarization
- Results <2KB pass through unchanged
- Graceful degradation on Gemini failure
- Semantic cache hit on similar queries

**Tools:**
- `semantic_search` finds relevant artifacts
- `get_artifact_context` respects token budget
- TTL enforcement (expired artifacts excluded)

### Integration Tests

**Full Workflow:**
1. Execute large query → returns summary + artifact ID
2. `semantic_search` finds the artifact
3. `get_artifact_context` retrieves full summary
4. Verify token reduction ≥70%

## Slash Command Injection

**`/gateway-init`** injects this into `.claude/CLAUDE.md`:

```markdown
## MCP Gateway Usage

### Direct Tools (call as MCP tools, NOT via run_code)
- `list_servers()` - Show connected servers
- `search_workspace({tags?, query?})` - Find saved artifacts
- `get_workspace_file({path})` - Read artifact (max 8K chars)
- `run_code({code})` - Execute TypeScript for OTHER servers
- `semantic_search({query, limit?})` - Find past results via NLP
- `get_artifact_context({ids, maxTokens?})` - Assemble context from artifacts
- Tupl: `wrap_agent`, `list_rule_families`, `get_telemetry`, `get_agent_config`

### run_code Pattern (for perplexity, context7, filesystem, etc.)
```typescript
run_code({code: `
  const data = await callMCPTool('perplexity', 'deep_research', {query: '...'});
  // Large results auto-summarized (≥2KB) via Gemini 2.5 Flash
  return data;
`})
```

### Auto-Summarization (≥2KB results)
Gateway automatically:
1. Checks semantic cache (3h TTL) for similar queries
2. If cache miss: Gemini generates summary (key findings + insights)
3. Returns summary + artifact reference
4. Stores vectors in ChromaDB for semantic_search

### Intelligence Workflow
```typescript
// 1. Query (auto-summarized if large)
run_code({code: `await callMCPTool('perplexity', 'search', {query: '...'})`})

// 2. Find related past work
semantic_search({query: 'authentication patterns', limit: 3})

// 3. Assemble focused context
get_artifact_context({ids: ['art-123'], maxTokens: 4000})
```

### Limits
- Summarization threshold: 2KB
- Return limit: 10K chars
- Artifact TTL: 3 hours
- Graceful degradation on AI failures (returns raw)
```

## Error Handling

### Failure Modes

1. **Gemini API rate limit (429)**
   - Catch error, log warning
   - Return raw result (subject to 10KB limit)

2. **ChromaDB collection missing**
   - Auto-create on first use
   - Continue normally

3. **Invalid JSON from Gemini**
   - Catch parse error
   - Return raw result

4. **Disk full (artifact storage)**
   - Return summary without persisting
   - Log error

5. **Missing `GEMINI_API_KEY`**
   - Disable intelligence layer at startup
   - Log warning, continue with raw mode

### Graceful Degradation

Every intelligence operation wraps in try-catch. On failure:
- Log error with context
- Return raw result
- Never throw to user

## Success Metrics

**Primary:**
- Token reduction ≥70% for results ≥5KB

**Secondary:**
- Graceful degradation: 100% (no user-facing errors)
- Cache hit rate: Track and log (target 20-30%)
- Latency overhead: <2s for summarization

## Implementation Checklist

**Phase 1: Foundation (Core Intelligence)**
- [ ] Add dependencies to package.json
- [ ] Create src/prompts/summarize.ts
- [ ] Modify src/runtime/sandbox.ts - add processWithIntelligence()
- [ ] Initialize ChromaDB in src/gateway.ts
- [ ] Validate GEMINI_API_KEY on startup
- [ ] Write unit tests for summarization

**Phase 2: MCP Tools (Semantic Layer)**
- [ ] Create src/mcp-server/intelligence-tools.ts
- [ ] Add semantic_search handler
- [ ] Add get_artifact_context handler
- [ ] Implement token estimation helper
- [ ] Write unit tests for tools

**Phase 3: Slash Command (DX)**
- [ ] Create src/mcp-server/commands/gateway-init.ts
- [ ] Implement CLAUDE.md injection
- [ ] Create .claude/commands/gateway-init.md
- [ ] Add command handler in MCPGatewayServer
- [ ] Test on fresh project

**Phase 4: Integration & Polish**
- [ ] Write tests/intelligence-e2e.test.ts
- [ ] Add token reduction metrics
- [ ] Test graceful degradation
- [ ] Test semantic cache scenarios
- [ ] Update README.md
- [ ] Document environment variables

## References

- Google GenAI SDK: `@google/genai` (NOT `@google/generative-ai`)
- ChromaDB: `chromadb` npm package
- Gemini Model: `gemini-2.5-flash`
- Structured outputs: `responseMimeType: 'application/json'`
